{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    infile = open(filename,'rb')\n",
    "    opened_file = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return opened_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortlist_text_top_10(sorted_terms):\n",
    "    #function to shortlist / remove redundancy from the sorted top-n result\n",
    "    list_all_terms=[x[0].split() for x in sorted_terms]\n",
    "    len_index = {}\n",
    "    \n",
    "    #make list of index with key length of words\n",
    "    for i,item in enumerate(list_all_terms):\n",
    "\n",
    "        if len(item) not in len_index:\n",
    "            len_index[len(item)] = [i]\n",
    "        else:\n",
    "            len_index[len(item)].append(i)\n",
    "    \n",
    "    begins_with = ['the','a','an','of']\n",
    "    ends_with = ['is']\n",
    "\n",
    "    remove_begin = []\n",
    "    idx_begins_ends = []\n",
    "    for i,x in enumerate(list_all_terms):\n",
    "        if x[0] in begins_with:\n",
    "            remove_begin.append(x[1:])\n",
    "            idx_begins_ends.append(i)\n",
    "        else:\n",
    "            remove_begin.append(x)\n",
    "                       \n",
    "    remove_begin_end = []\n",
    "    for i,x in enumerate(remove_begin):\n",
    "        if x[-1] in ends_with:\n",
    "            remove_begin_end.append(x[:-1])\n",
    "            idx_begins_ends.append(i)\n",
    "        else:\n",
    "            remove_begin_end.append(x)\n",
    "    \n",
    "    \n",
    "    #remove_the = [x[1:] if x[0] in begins_with else x for x in list_all_terms] #Remove 'the','a','an' in candidate terms\n",
    "    \n",
    "    to_be_remove = []\n",
    "    for j,item in enumerate(remove_begin_end):\n",
    "        for i in range(len(remove_begin_end)):\n",
    "            if (item == remove_begin_end[i]) & (i!=j):\n",
    "                if i in idx_begins_ends:\n",
    "                    to_be_remove.append(i)\n",
    "    \n",
    "    to_be_remove = sorted(set(to_be_remove)) #for keeeping the index number for items to be remove\n",
    "            \n",
    "    sorted_length = sorted(len_index.keys())\n",
    "    max_len = sorted_length[-1]\n",
    "    \n",
    "    keep_counter = 0\n",
    "\n",
    "    for i,candidate in enumerate(list_all_terms):\n",
    "        if i not in to_be_remove:\n",
    "            keep_counter = keep_counter + 1\n",
    "            if i!=0:\n",
    "                len_current = len(candidate)\n",
    "                # check previous index which lenght is lower and not in to_be_remove list\n",
    "                if len_current!=1:\n",
    "                    for lower_len in range(1,len_current):\n",
    "                        lower_len_indexes = len_index.get(lower_len,[]) #get indexes for lower length terms\n",
    "\n",
    "                        for check_index in lower_len_indexes:\n",
    "                            if (check_index<i) & (check_index not in to_be_remove):\n",
    "                                delete_flag = True\n",
    "                                for check_term in list_all_terms[check_index]:\n",
    "                                    if check_term in candidate:\n",
    "                                        delete_flag=delete_flag & True\n",
    "                                    else:\n",
    "                                        delete_flag = delete_flag & False\n",
    "\n",
    "                                if delete_flag== True: #if the terms or multi terms are in the multi terms then flag to be deleted\n",
    "                                    to_be_remove.append(check_index)\n",
    "                                    keep_counter = keep_counter - 1\n",
    "\n",
    "                #Check previous index which length is higher and not in to_be_remove list\n",
    "                if len_current!=max_len:\n",
    "                    for higher_len in range(len_current+1,max_len+1):\n",
    "                        higher_len_indexes = len_index.get(higher_len,[])\n",
    "\n",
    "                        for check_index in higher_len_indexes:\n",
    "                            if (check_index<i) & (check_index not in to_be_remove):\n",
    "                                delete_flag = True\n",
    "                                previous_term = list_all_terms[check_index] #previous index term with higher length\n",
    "                                for curr_term in candidate:\n",
    "                                    if curr_term in previous_term:\n",
    "                                        delete_flag = delete_flag & True\n",
    "                                    else:\n",
    "                                        delete_flag = delete_flag & False\n",
    "\n",
    "                                if delete_flag == True:\n",
    "                                    to_be_remove.append(i)\n",
    "                                    keep_counter = keep_counter-1\n",
    "                                    \n",
    "        if keep_counter == 10:\n",
    "            break\n",
    "       \n",
    "    keep_terms = [sorted_terms[x] for x in range(len(list_all_terms)) if x not in to_be_remove][0:10]\n",
    "    \n",
    "    \n",
    "    return keep_terms #Index of the terms being kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data related to tag\n",
    "tags = load_pickle('Data/tagged_tags_dictionary_withsents_film(available_in_index).pkl')\n",
    "mapped_tags = load_pickle('Data/mapped_tags_to_group_film(available_in_index).pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load saved sorted terms by log likelihood\n",
    "sorted_terms_unigram = load_pickle('Data/sorted_terms/sorted_terms_tag_single_shingle_unigram_top30.pkl')\n",
    "sorted_terms_unigram_multi = load_pickle('Data/sorted_terms/sorted_terms_tag_multi_shingle_unigram_top30.pkl')\n",
    "\n",
    "sorted_terms_bigram = load_pickle('Data/sorted_terms/sorted_terms_tag_single_shingle_max2_top30.pkl')\n",
    "sorted_terms_bigram_multi = load_pickle('Data/sorted_terms/sorted_terms_tag_multi_shingle_max2_top30.pkl')\n",
    "\n",
    "sorted_terms_4gram = load_pickle('Data/sorted_terms/sorted_terms_tag_single_shingle_max4_top30.pkl')\n",
    "sorted_terms_4gram_multi = load_pickle('Data/sorted_terms/sorted_terms_tag_multi_shingle_max4_top30.pkl')\n",
    "\n",
    "for key in sorted_terms_unigram_multi:\n",
    "    sorted_terms_unigram[key] = sorted_terms_unigram_multi[key]\n",
    "    sorted_terms_bigram[key] = sorted_terms_bigram_multi[key]\n",
    "    sorted_terms_4gram[key] = sorted_terms_4gram_multi[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select random adjective tags\n",
    "adjective_tags = random.sample(mapped_tags['adjective'],30)\n",
    "adjective_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select random non-adjective tags\n",
    "mapped_non_adjective = []\n",
    "for key in mapped_tags:\n",
    "    if key != 'adjective':\n",
    "        mapped_non_adjective = mapped_non_adjective + mapped_tags[key]\n",
    "non_adjective_tags = random.sample(mapped_non_adjective,50)\n",
    "non_adjective_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the top10 results for adjective and non adjective to tsv files for analysis of synonym\n",
    "\n",
    "#out_file1 = open(\"Data/synonym_analysis/adjective_sorted_terms_random.tsv\", 'wt')\n",
    "out_file2 = open(\"Data/synonym_analysis/non_adjective_sorted_terms_random.tsv\", 'wt') \n",
    "\n",
    "#with open(\"Data/sorted_terms/top_5_terms_per_tag(above_5).tsv\", 'wt') as out_file:\n",
    "#tsv_writer1 = csv.writer(out_file1, delimiter='\\t')\n",
    "tsv_writer2 = csv.writer(out_file2, delimiter='\\t')\n",
    "#tsv_writer1.writerow([\"Tag\"] + ['rank'+str(i) for i in range(1,11)])\n",
    "tsv_writer2.writerow([\"Tag\"] + ['rank'+str(i) for i in range(1,11)])\n",
    "\n",
    "for i,tag in enumerate(adjective_tags):\n",
    "    sorted_terms_tag = shortlist_text_top_10(sorted_terms_4gram[tag])\n",
    "    #tsv_writer1.writerow([tag] + [x[0] for x in sorted_terms_tag])\n",
    "#out_file1.close()\n",
    "for i,tag in enumerate(non_adjective_tags):\n",
    "    sorted_terms_tag = shortlist_text_top_10(sorted_terms_4gram[tag])\n",
    "    tsv_writer2.writerow([tag] + [x[0] for x in sorted_terms_tag])\n",
    "out_file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
