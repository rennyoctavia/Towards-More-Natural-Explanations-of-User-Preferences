{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Template-Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /Users/renny/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "nltk.download('tagsets')\n",
    "from nltk.data import load\n",
    "from nltk.corpus import conll2000\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "\n",
    "def dump_pickle(filename,obj):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(obj,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_preference = [('+','++'),('-','--'),('++','+'),('--','-'),('+','-'),('-','+'),('N','+'),('N','-')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('output/tagged_tags_dictionary_withsents_film.pkl','rb') as f:\n",
    "train_tags = load_pickle('data/dataset/splitted_tags/train_tags.pkl')\n",
    "test_tags = load_pickle('data/dataset/splitted_tags/test_tags.pkl')\n",
    "mapped_train_tags = load_pickle('data/dataset/splitted_tags/mapped_train_tags.pkl')\n",
    "mapped_test_tags = load_pickle('data/dataset/splitted_tags/mapped_test_tags.pkl')\n",
    "cleaned_train_tags = train_tags.keys()\n",
    "cleaned_test_tags = test_tags.keys()\n",
    "train_synonym_dictionary = load_pickle('data/tags_synonym/top_10_synonym_train.pkl') #list of synonym from train tags\n",
    "\n",
    "#Load all tags and mapped all tags\n",
    "all_tags = load_pickle('data/dataset/all_tags/tagged_tags_dictionary_withsents_film.pkl')\n",
    "mapped_all_tags = load_pickle('data/dataset/all_tags/mapped_tags_to_group_film')\n",
    "cleaned_all_tags = list(all_tags.keys())\n",
    "\n",
    "like_synonyms_basic = [\"like\"]\n",
    "dont_like_synonyms_basic = [\"don't like\"]\n",
    "like_synonyms=[\"like\",\"love\",\"prefer\",\"enjoy\",\"are into\", \"would watch\", \"like to watch\",\"like watching\", \"love to watch\",\"love watching\",\"prefer to watch\",\"prefer watching\",\"enjoy watching\",\"are interested in\"]\n",
    "dont_like_synonyms=[\"dislike\",\"don't like\",\"hate\",\"don't prefer\",\"are not into\", \"wouldn't watch\",\"dislike watching\",\"don't like to watch\",\"don't like watching\",\"hate to watch\",\"hate watching\",\"don't prefer watching\",\"don't prefer to watch\",\"are not interested in\"]\n",
    "especially_synonyms = [\"especially\", \"particularly\"]\n",
    "unless_synonyms = [\"unless\",\"except if\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic_1 templates\n",
    "def get_summary_basic_1(tag1,tag2):\n",
    "    \n",
    "    if len(tag1[1]) ==1:\n",
    "        \n",
    "        if len(tag2[1])==2:\n",
    "            #1st template (+,++) or (-,--)\n",
    "            text1 = random.choice(like_synonyms_basic) if tag1[1]=='+' else random.choice(dont_like_synonyms_basic)\n",
    "            return \"You {} [{}] movies, especially if they are [{}].\".format(text1,tag1[0],tag2[0])\n",
    "            \n",
    "        else:\n",
    "            #4th template (N,+) or (N,-)\n",
    "            if tag1[1] == 'N':\n",
    "                text1 = random.choice(like_synonyms_basic) if tag2[1]=='+' else random.choice(dont_like_synonyms_basic)\n",
    "                return \"You {} [{}] movies if they are [{}].\".format(text1,tag1[0],tag2[0])\n",
    "    \n",
    "            #3rd template (+,-) or (-,+)    \n",
    "            else:\n",
    "                    text1 = random.choice(like_synonyms_basic) if tag1[1]=='+' else random.choice(dont_like_synonyms_basic)\n",
    "                    return \"You {} [{}] movies, unless they are [{}].\".format(text1,tag1[0],tag2[0])\n",
    "    \n",
    "    #2nd template (++,+) or (--,-)\n",
    "    else:\n",
    "        text1 = random.choice(like_synonyms_basic) if tag1[1]=='++' else random.choice(dont_like_synonyms_basic)\n",
    "        return \"You {} [{}] movies, especially if they are not [{}].\".format(text1,tag1[0],tag2[0])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic template adapted from the paper : \n",
    "1. You (don't) like {tag1} movies especially if they are {tag2} -- for ((tag1,+), (tag2,++)) or ((tag1,-),(tag2,--))\n",
    "2. You (don't) like {tag1} movies especially if they are not {tag2} -- for ((tag1,++),(tag2,+)) or ((tag1,--),(tag2,-))\n",
    "3. You (don't) like {tag1} movies unless they are {tag2} -- for ((tag1,+),(tag2,-)) or ((tag1,-),(tag2,+))\n",
    "4. You (don't) like {tag1} movies if they are {tag2} -- for ((tag1,N),(tag2,+)) or ((tag1,N),(tag2,-))\n",
    "\n",
    "\n",
    "### Function get_summary\n",
    "This function is to generate phrase based on above basic template, it will take tuple of tag1 and tag2, where each contain a pair of text and the sign (+,++,-, etc.), and return the phrase.\n",
    "\n",
    "In this function, it will generate sentence 1 and sentence 2 by calling function get_first_sentence, and get_second_sentence. The generated sentence 1 and 2 will have similar meaning with the sentence it replaced, it is just can have different pattern. The tag text will not be changed.\n",
    "\n",
    "Example from the first basic template:\n",
    "\n",
    "<img src=\"screenshot.png\" width=\"400\" height=\"200\" >\n",
    "\n",
    "\n",
    "### Function get_first_sentence\n",
    "It is to generate sentence 1 part, by picking random pattern which is basically different ways of saying the basic form. \n",
    "In each sentence pattern, it will also pick randomly different words for expressing like or don't like.\n",
    "The pattern for the sentence is manually picked (there are 3 patterns), and the different expresison of like/don't like is also manually picked.\n",
    "\n",
    "### Function get_second_sentence\n",
    "It is to generate sentence 2 part, with the same way of get_first_sentence function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic_2 templates\n",
    "\n",
    "def get_first_sentence(tag,dictionary_tags,sentiment = 1):\n",
    "    group = dictionary_tags[tag]['POS_Tags_Group']\n",
    "    entity = dictionary_tags[tag]['Entity']  \n",
    "    person_type = dictionary_tags[tag]['Person_Type']\n",
    "    end_with_movie = dictionary_tags[tag]['End_with_movie']\n",
    "    start_pos = dictionary_tags[tag]['Start_POS']\n",
    "    end_pos = dictionary_tags[tag]['End_POS']\n",
    "    genre = dictionary_tags[tag]['genre']\n",
    "    \n",
    "    #if end with movie/movies/film/films\n",
    "    if end_with_movie == True:\n",
    "         statement_patterns = [\"You {} [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag)]#, #Basic pattern\n",
    "            \n",
    "    elif entity == 'GPE':\n",
    "        statement_patterns = [\"You {} movies about [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag),\n",
    "                             \"You {} movies from [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag)]\n",
    "    elif genre==True:\n",
    "        statement_patterns = [\"You {} [{}] movies\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag),\n",
    "                             \"You {} movies with [{}] {}\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag,random.choice(['genre','content'])),\n",
    "                             \"You {} movies full of [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag)]\n",
    "        \n",
    "    \n",
    "    #elif tag.startswith('oscar'): ## Since oscar is a famous award, make own rule for it\n",
    "    #    statement_patterns = [\"You {} '{}' movies\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag)]\n",
    "    \n",
    "    elif ((group == 'noun') | (group == 'numeral')) & (not tag.startswith(\"oscar\")):\n",
    "        if entity == 'PERSON':\n",
    "            if person_type == 'actor':\n",
    "                  statement_patterns = [\"You {} movies starred by [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag), #Basic pattern\n",
    "                                       \"You {} movies starring [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag),   \n",
    "                                        \"You {} movies played by [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag),\n",
    "                                        \"[{}] movies is {}for you\".format(tag.capitalize(), \"\" if sentiment==1 else \"not \")]\n",
    "            elif (person_type == 'director'):\n",
    "                statement_patterns = [\"You {} movies directed by [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag), #Basic pattern\n",
    "                          \"[{}] movies is {}for you\".format(tag.capitalize(), \"\" if sentiment==1 else \"not \")]\n",
    "            else:\n",
    "                statement_patterns = [\"You {} movies about [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag)]\n",
    "    \n",
    "        elif ((start_pos.startswith('JJ')) | (start_pos=='DT')) & (end_pos=='VBG'):\n",
    "            statement_patterns = [\"You {} movies with [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag)]\n",
    "    \n",
    "        else:\n",
    "            statement_patterns = [\"You {} movies about [{}]\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag)]\n",
    "    \n",
    "\n",
    "    else : #adjective & other\n",
    "        statement_patterns = [\"You {} [{}] movies\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag), #Basic pattern\n",
    "                              \"[{}] movies is {}for you\".format(tag.capitalize(), \"\" if sentiment==1 else \"not \")]\n",
    "                              #  \"You {} movies filled with '{}'\".format(random.choice(like_synonyms) if sentiment == 1 else random.choice(dont_like_synonyms),tag)]\n",
    "\n",
    "    return(random.choice(statement_patterns))\n",
    "\n",
    "def get_second_sentence(tag,dictionary_tags,sentiment = 1):\n",
    "    group = dictionary_tags[tag]['POS_Tags_Group']\n",
    "    entity = dictionary_tags[tag]['Entity']\n",
    "    person_type = dictionary_tags[tag]['Person_Type']\n",
    "    end_with_movie =dictionary_tags[tag]['End_with_movie']\n",
    "    start_pos =dictionary_tags[tag]['Start_POS']\n",
    "    end_pos =dictionary_tags[tag]['End_POS']\n",
    "    genre = dictionary_tags[tag]['genre']\n",
    "    \n",
    "    \n",
    "    if end_with_movie == True:\n",
    "        statement_patterns = [\"they are {}[{}]\".format(\"\" if sentiment==1 else \"not \",tag)]#,#Basic pattern for second sentence\n",
    "    \n",
    "    elif entity == 'GPE':\n",
    "        statement_patterns = [\"they are {}about [{}]\".format(\"\" if sentiment==1 else \"not \",tag),\n",
    "                             \"they are {}from [{}]\".format(\"\" if sentiment==1 else \"not \",tag)]\n",
    "    \n",
    "    elif tag.startswith('oscar'): ## Since oscar is a famous award, make own rule for it\n",
    "        statement_patterns = [\"they are {}[{}] movies\".format(\"\" if sentiment==1 else \"not \",tag)]\n",
    "  \n",
    "    elif genre == True:\n",
    "        statement_patterns = [\"they are {}full of [{}]\".format(\"\" if sentiment==1 else \"not \",tag),\n",
    "                             \"they {} [{}]\".format(\"contain\" if sentiment==1 else \"don't contain\",tag),\n",
    "                             \"they are {}[{}]\".format(\"\" if sentiment==1 else \"not \",tag)]\n",
    "    \n",
    "    elif (group == 'noun') | (group == 'numeral'):\n",
    "        if entity == 'PERSON':\n",
    "            if person_type == 'actor':\n",
    "                  statement_patterns = [\"they are {}starred by [{}]\".format(\"\" if sentiment==1 else \"not \",tag), #Basic pattern\n",
    "                          \"they are {}starring [{}]\".format(\"\" if sentiment==1 else \"not \",tag),\n",
    "                          \"they are {}played by [{}]\".format(\"\" if sentiment==1 else \"not \",tag)]\n",
    "            elif (person_type == 'director'):\n",
    "                statement_patterns = [\"they are {}directed by [{}]\".format(\"\" if sentiment==1 else \"not \",tag),\n",
    "                                      \"they are {}movies by [{}]\".format(\"\" if sentiment==1 else \"not \",tag)]\n",
    "            else:\n",
    "                statement_patterns = [\"they are {}about [{}]\".format(\"\" if sentiment==1 else \"not \",tag),\n",
    "                                 \"they are {}[{}] movies\".format(\"\" if sentiment==1 else \"not \",tag)]\n",
    "        \n",
    "        elif ((start_pos.startswith('JJ')) | (start_pos=='DT')) & (end_pos=='VBG'):\n",
    "            statement_patterns = [\"they are {}with [{}]\".format(\"\" if sentiment==1 else \"not \",tag)]\n",
    "                \n",
    "        else:\n",
    "            statement_patterns = [\"they are {}about [{}]\".format(\"\" if sentiment==1 else \"not \",tag)]\n",
    "    \n",
    "\n",
    "    else : #adjectives and others, also if end_with_movie == True\n",
    "    # Below are manually set pattern for first sentence with positive or negative sentiment which can be randomly chosen\n",
    "        statement_patterns = [\"they are {}[{}]\".format(\"\" if sentiment==1 else \"not \",tag)]#,#Basic pattern for second sentence\n",
    "                         # $\"they are {}full of '{}'\".format(\"\" if sentiment==1 else \"not \", tag),\n",
    "                        #\"they {} high '{}' content\".format(\"have\" if sentiment==1 else \"don't have\",tag)]\n",
    "   \n",
    "    \n",
    "    \n",
    "    return(random.choice(statement_patterns))\n",
    "\n",
    "def get_summary_basic_2(tag1,tag2,dictionary_tags):\n",
    "    \" Function to generate template_based summary/phrase, using basic template as above explanation in markdown\"\n",
    "    \n",
    "    if len(tag1[1]) ==1:\n",
    "        \n",
    "        if len(tag2[1])==2:\n",
    "            #1st template (+,++) or (-,--)\n",
    "            sentiment_1 = 1 if tag1[1]=='+' else 0\n",
    "            return \"{}, {} if {}.\".format(get_first_sentence(tag1[0],dictionary_tags,sentiment_1),random.choice(especially_synonyms),get_second_sentence(tag2[0],dictionary_tags)) #especially\n",
    "            \n",
    "        else:\n",
    "            #4th template (N,+) or (N,-)\n",
    "            if tag1[1] == 'N':\n",
    "                sentiment_1 = 1 if tag2[1]=='+' else 0\n",
    "                return \"{} if {}.\".format(get_first_sentence(tag1[0],dictionary_tags,sentiment_1),get_second_sentence(tag2[0],dictionary_tags))\n",
    "    \n",
    "            #3rd template (+,-) or (-,+)    \n",
    "            else:\n",
    "                    sentiment_1 = 1 if tag1[1]=='+' else 0\n",
    "                    return \"{}, {} {}.\".format(get_first_sentence(tag1[0],dictionary_tags,sentiment_1),random.choice(unless_synonyms),get_second_sentence(tag2[0],dictionary_tags)) #unless\n",
    "    \n",
    "    #2nd template (++,+) or (--,-)\n",
    "    else:\n",
    "        sentiment_1 = 1 if tag1[1]=='++' else 0\n",
    "        return \"{}, {} if {}.\".format(get_first_sentence(tag1[0],dictionary_tags,sentiment_1),random.choice(especially_synonyms),get_second_sentence(tag2[0],dictionary_tags,0))#especially\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get N preference statements\n",
    "To generate n number of preference statement, with random tags and random preferences level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preference_statements(dictionary_tags,pair_tags_list=None,list_pair_preferences=None, num_sentence = 200): \n",
    "    '''\n",
    "    param num_statements : number of statements to be generated\n",
    "    param tags : list of tags. From this list, the tags for each statement will be randomly choosen\n",
    "    '''\n",
    "    list_preference_statements_basic = []\n",
    "    list_preference_statements_adjusted = []\n",
    "    list_preference = []\n",
    "    list_tags = []\n",
    "    list_postags = []\n",
    "    \n",
    "    \n",
    "    if pair_tags_list == None:\n",
    "        pair_tags_list =[]\n",
    "        list_pair_preferences = []\n",
    "        for i in range(num_sentence):\n",
    "            pair_tags_list.append(random.sample(cleaned_all_tags,k=2))\n",
    "            list_pair_preferences.append(random.choice(list_of_preference))\n",
    "            \n",
    "\n",
    "    for pair_tags,pair_preferences in zip(pair_tags_list,list_pair_preferences):\n",
    "        #pair_tags = random.sample(tags,k=2)\n",
    "        list_postags.append([dictionary_tags[pair_tags[0]]['POS_Tags'],dictionary_tags[pair_tags[1]]['POS_Tags']])\n",
    "        #pair_preferences = random.choice(list_of_preference)\n",
    "        pref_statement_basic = get_summary_basic_1((pair_tags[0],pair_preferences[0]),(pair_tags[1],pair_preferences[1]))\n",
    "        pref_statement_adjusted = get_summary_basic_2((pair_tags[0],pair_preferences[0]),(pair_tags[1],pair_preferences[1]),dictionary_tags)\n",
    "        #print (pref_statement)\n",
    "        list_preference_statements_basic.append(pref_statement_basic)\n",
    "        list_preference_statements_adjusted.append(pref_statement_adjusted)\n",
    "        list_preference.append(pair_preferences)\n",
    "        list_tags.append(pair_tags)\n",
    "    return list_preference_statements_basic,list_preference_statements_adjusted, list_preference,list_tags, list_postags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Other function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(title,list_preference_statements, list_preference,list_tags, list_postags):   \n",
    "    '''\n",
    "    Saving the generated statements into excel file\n",
    "    '''\n",
    "    # Write the generated sentences in a csv file for review\n",
    "    list_tags = np.array(list_tags)\n",
    "    list_preference = np.array(list_preference)\n",
    "    list_postags = np.array(list_postags)\n",
    "    \n",
    "    pd.DataFrame({'Preference_statement': list_preference_statements,\n",
    "                  'tag1': list_tags[0:,0],\n",
    "                  'pos-tag1' : list_postags[0:,0],\n",
    "                  'pref1': list_preference[0:,0],\n",
    "                  'tag2': list_tags[0:,1],\n",
    "                  'pos-tag2' : list_postags[0:,1],\n",
    "                  'pref2': list_preference[0:,1]\n",
    "    }).to_excel(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to save train data into several format, to suit the pre-trained models\n",
    "\n",
    "def save_to_txt(title,list_sentence):\n",
    "    MyFile=open(title,'w')\n",
    "\n",
    "    for element in list_sentence:\n",
    "        MyFile.write(element.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "    MyFile.close()\n",
    "\n",
    "def save_train_data(title,ori,para1,para2,para3):\n",
    "    MyFile=open(title,'w',encoding='UTF-8')\n",
    "\n",
    "    for ori,para1,para2,para3 in zip(ori,para1,para2,para3):\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para1.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para2.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para3.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "    MyFile.close()\n",
    "    \n",
    "def save_train_data_1(title,ori,para1):\n",
    "    MyFile=open(title,'w',encoding='UTF-8')\n",
    "\n",
    "    for ori,para1 in zip(ori,para1):\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para1.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "    MyFile.close()\n",
    "    \n",
    "def save_train_data_2(title,ori,para1,para2,para3):\n",
    "    MyFile=open(title,'w',encoding='UTF-8')\n",
    "\n",
    "    for ori,para1,para2,para3 in zip(ori,para1,para2,para3):\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para1.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para2.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para3.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "    MyFile.close()\n",
    "\n",
    "def save_test_data(title,list_sentence_basic):\n",
    "    MyFile=open(title,'w')\n",
    "\n",
    "    for ori in list_sentence_basic:\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>')\n",
    "        MyFile.write('\\n')\n",
    "    MyFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate pair of tags:\n",
    "import numpy as np\n",
    "first_tags = []\n",
    "second_tags = []\n",
    "synonym_first_tags = []\n",
    "synonym_second_tags = []\n",
    "pair_tags = [] #for for ori and paraphrase 1\n",
    "\n",
    "number_of_sentences = 100000\n",
    "\n",
    "for i in range(number_of_sentences):\n",
    "    pair= random.sample(cleaned_train_tags,k=2) # get data from train split\n",
    "    pair_tags.append(pair)\n",
    "    first_tags.append(pair[0])\n",
    "    synonym_first_tags.append(np.random.choice(list(train_synonym_dictionary[pair[0]].keys())[0:3],p=[0.7,0.2,0.1]))\n",
    "    second_tags.append(pair[1])\n",
    "    synonym_second_tags.append(np.random.choice(list(train_synonym_dictionary[pair[1]].keys())[0:3],p=[0.7,0.2,0.1]))\n",
    "    \n",
    "pair_tags_para2 = list(zip(synonym_first_tags,second_tags))\n",
    "pair_tags_para3 = list(zip(first_tags,synonym_second_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump_pickle('data/dataset/train_data_2/pair_tags_ori_100k.pkl',pair_tags)\n",
    "#dump_pickle('data/dataset/train_data_2/pair_tags_para1_100k.pkl',pair_tags)\n",
    "#dump_pickle('data/dataset/train_data_2/pair_tags_para2_100k.pkl',pair_tags_para2)\n",
    "#dump_pickle('data/dataset/train_data_2/pair_tags_para3_100k.pkl',pair_tags_para3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re generate preference statements for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_preference_statements_basic,list_preference_statements_adjusted, list_preference, list_tags, list_postags = generate_preference_statements(all_tags)\n",
    "save_to_file(\"basic_user_preferences.xlsx\",list_preference_statements_basic, list_preference,list_tags, list_postags) \n",
    "save_to_file(\"adjusted_user_preferences.xlsx\",list_preference_statements_adjusted, list_preference,list_tags, list_postags)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Preference statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load selected tags and pair user preferences which have been selected previously to generate statements using both template, and using the same tags and pairs user preference\n",
    "pair_tags = load_pickle('data/dataset/train_data_2/pair_tags_ori_50k.pkl')\n",
    "pair_tags = load_pickle('data/dataset/train_data_2/pair_tags_para1_50k.pkl')\n",
    "pair_tags_para2=load_pickle('data/dataset/train_data_2/pair_tags_para2_50k.pkl')\n",
    "pair_tags_para3 = load_pickle('data/dataset/train_data_2/pair_tags_para3_50k.pkl')\n",
    "list_pair_preferences = load_pickle('data/dataset/train_data_2/pair_preferences_50k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate preference statements with all filtered tags\n",
    "list_preference_statements_basic,list_preference_statements_adjusted, list_preference, list_tags, list_postags = generate_preference_statements(train_tags,pair_tags_para3,list_pair_preferences)\n",
    "#save_to_file('data/dataset/train_data_2/preference_statement_train2_basic_50k.xlsx',list_preference_statements_basic, list_preference,list_tags,list_postags)\n",
    "save_to_file('data/dataset/train_data_2/preference_statement_train2_para3_50k.xlsx',list_preference_statements_adjusted, list_preference,list_tags,list_postags)\n",
    "#dump_pickle('data/dataset/train_data_2/list_preference_statements_train2_basic_50k.pkl',list_preference_statements_basic)\n",
    "dump_pickle('data/dataset/train_data_2/list_preference_statements_train2_para3_50k.pkl',list_preference_statements_adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_basic_100k.pkl')\n",
    "para1 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para1_100k.pkl')\n",
    "para2 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para2_100k.pkl')\n",
    "para3 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para3_100k.pkl')\n",
    "\n",
    "#save_train_data('data/dataset/train_data_2/utf_8_train_data/train_2_100k_ver1.txt',ori,para1,para2,para3)\n",
    "save_train_data_2('data/dataset/train_data_2/utf_8_train_data/train_2_100k_ver2.txt',ori,para1,para2,para3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_basic_100k.pkl')\n",
    "para1 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para1_100k.pkl')\n",
    "#para2 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para2_10k.pkl')\n",
    "#para3 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para3_10k.pkl')\n",
    "\n",
    "save_train_data_1('data/dataset/train_data_1/New train1 dataset/train_1_100k.txt',ori,para1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generate test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_data = 2000\n",
    "pair_test_preferences = []\n",
    "pair_test_tags = []\n",
    "\n",
    "for i in range(n_test_data):\n",
    "    pair_test_tags.append(random.sample(cleaned_test_tags,k=2))\n",
    "    pair_test_preferences.append(random.choice(list_of_preference))\n",
    "dump_pickle('data/dataset/test_data/pair_test_tags_2000.pkl',pair_test_tags)\n",
    "dump_pickle('data/dataset/test_data/pair_test_preferences_2000.pkl',pair_test_preferences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_preference_statements_basic,list_preference_statements_adjusted, list_preference, list_tags, list_postags = generate_preference_statements(test_tags,pair_test_tags,pair_test_preferences)\n",
    "save_to_file('data/dataset/test_data/preference_statement_test_basic_2000.xlsx',list_preference_statements_basic, list_preference,list_tags,list_postags)\n",
    "save_to_file('data/dataset/test_data/preference_statement_test_adjusted_2000.xlsx',list_preference_statements_adjusted, list_preference,list_tags,list_postags)\n",
    "dump_pickle('data/dataset/test_data/list_preference_statements_test_basic_2000.pkl',list_preference_statements_basic)\n",
    "dump_pickle('data/dataset/test_data/list_preference_statements_test_adjusted_2000.pkl',list_preference_statements_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_data('data/dataset/test_data/test_2000_basic.txt',list_preference_statements_basic)\n",
    "save_test_data('data/dataset/test_data/test_2000_adjusted.txt', list_preference_statements_adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate MSCOCO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"data/mscoco/train_source.txt\",'r')\n",
    "mscoco_source = file1.readlines()\n",
    "file2 = open(\"data/mscoco/train_target.txt\",'r')\n",
    "mscoco_target = file2.readlines()\n",
    "\n",
    "mscoco_source = [x.replace(\"\\n\",'') for x in mscoco_source]#[264930:]\n",
    "mscoco_target = [x.replace(\"\\n\",'') for x in mscoco_target]#[264930:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_train_data_1(\"data/mscoco/mscoco_val.txt\",mscoco_source,mscoco_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mixed_train_data_2(title,ori,para1,para2,para3,mscoco_source,mscoco_target):\n",
    "    MyFile=open(title,'w',encoding='UTF-8')\n",
    "\n",
    "    for ori,para1,para2,para3,mscoco_src,mscoco_tgt in zip(ori,para1,para2,para3,mscoco_source,mscoco_target):\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para1.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para2.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para3.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write(mscoco_src[0]+'>>>>>>'+mscoco_tgt[0])\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write(mscoco_src[1]+'>>>>>>'+mscoco_tgt[1])\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "    MyFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mixed_train_data_1(title,ori,para1,mscoco_source,mscoco_target):\n",
    "    MyFile=open(title,'w',encoding='UTF-8')\n",
    "\n",
    "    for ori,para1,mscoco_src,mscoco_tgt in zip(ori,para1,mscoco_source,mscoco_target):\n",
    "        MyFile.write(ori.replace('[',\"\").replace(']',\"\")+'>>>>>>'+para1.replace('[',\"\").replace(']',\"\"))\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write(mscoco_src+'>>>>>>'+mscoco_tgt)\n",
    "        MyFile.write('\\n')\n",
    "        MyFile.write('<|end of text|>')\n",
    "        MyFile.write('\\n')\n",
    "    MyFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Generate Mixed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_basic_50k.pkl')\n",
    "para1 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para1_50k.pkl')\n",
    "para2 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para2_50k.pkl')\n",
    "para3 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para3_50k.pkl')\n",
    "mscoco_source_10k = np.array(mscoco_source[0:100000]).reshape(50000,2)\n",
    "mscoco_target_10k = np.array(mscoco_target[0:100000]).reshape(50000,2)\n",
    "#save_train_data('data/dataset/train_data_2/utf_8_train_data/train_2_100k_ver1.txt',ori,para1,para2,para3)\n",
    "save_mixed_train_data_2('data/dataset/train_data_2/utf_8_train_data/mixed_train_2_50k_ver2.txt',ori,para1,para2,para3,mscoco_source_10k,mscoco_target_10k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_basic_50k.pkl')\n",
    "para1 = load_pickle('data/dataset/train_data_2/list_preference_statements_train2_para1_50k.pkl')\n",
    "mscoco_source_50k = mscoco_source[0:50000]\n",
    "mscoco_target_50k = mscoco_target[0:50000]\n",
    "#save_train_data('data/dataset/train_data_2/utf_8_train_data/train_2_100k_ver1.txt',ori,para1,para2,para3)\n",
    "save_mixed_train_data_1('data/dataset/train_data_1/New train1 dataset/mixed_train_1_50k.txt',ori,para1,mscoco_source_50k,mscoco_target_50k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate statements from template-based approac for Comparison with the neural paraphrase approach (for questionnaire)\n",
    "generate from test_20_final.txt , since this is not for training neural model, the synonym can be taken from all tags, since if it is only in the test tags then it wont resulted in good synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tagged_tags = load_pickle(\"data/output/tagged_tags_dictionary_withsents_film_17_05_20.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synonyms = load_pickle(\"data/tags_synonym/top_10_synonym_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair_test_data_synonym=[]\n",
    "for pair in pair_test_tags:\n",
    "    synonym_0 = np.random.choice(list(all_synonyms[pair[0]].keys())[0:3],p=[0.7,0.2,0.1])\n",
    "    synonym_1 = np.random.choice(list(all_synonyms[pair[1]].keys())[0:3],p=[0.7,0.2,0.1])\n",
    "    pair_test_data_synonym.append((synonym_0,synonym_1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_synonym_dictionary:\n",
    "    if \"scandal\" in i:\n",
    "        print (i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
