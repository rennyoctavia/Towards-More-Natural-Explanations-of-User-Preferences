{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42883,
     "status": "ok",
     "timestamp": 1591641374978,
     "user": {
      "displayName": "Renny Octavia Tan",
      "photoUrl": "",
      "userId": "17487398813778950254"
     },
     "user_tz": -120
    },
    "id": "YO_sDZW-JN0W",
    "outputId": "f16e8638-d326-4b15-b57e-84a68cbd5242"
   },
   "source": [
    "# Generate Praphrase from fine tuned T5 models\n",
    "\n",
    "We modified code from https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb#scrollTo=zrtR2urJV3ST for paraphrasing, which is originally a tutorial for fine-tuning pre-trained T5 for closed book question by the T5 author.\n",
    "\n",
    "This notebook is to be run in Google Colab using TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "%tensorflow_version 2.x\n",
    "!pip install -q t5\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import t5\n",
    "\n",
    "BASE_DIR = \"gs://t5_renny_new\" #@param { type: \"string\" }\n",
    "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
    "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data_paraphrase\")\n",
    "ON_CLOUD = True\n",
    "\n",
    "\n",
    "if ON_CLOUD:\n",
    "  print(\"Setting up GCS access...\")\n",
    "  import tensorflow_gcs_config\n",
    "  from google.colab import auth\n",
    "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
    "  TPU_TOPOLOGY = \"2x2\"\n",
    "  try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    TPU_ADDRESS = tpu.get_master()\n",
    "    print('Running on TPU:', TPU_ADDRESS)\n",
    "  except ValueError:\n",
    "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "  auth.authenticate_user()\n",
    "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
    "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Improve logging.\n",
    "from contextlib import contextmanager\n",
    "import logging as py_logging\n",
    "\n",
    "if ON_CLOUD:\n",
    "  tf.get_logger().propagate = False\n",
    "  py_logging.root.setLevel('INFO')\n",
    "\n",
    "@contextmanager\n",
    "def tf_verbosity_level(level):\n",
    "  og_level = tf.logging.get_verbosity()\n",
    "  tf.logging.set_verbosity(level)\n",
    "  yield\n",
    "  tf.logging.set_verbosity(og_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcoT7tOYPgqH"
   },
   "outputs": [],
   "source": [
    "original_1 = \"You like robbery movies, especially if they are keri russell.\" #@param {type:\"string\"}\n",
    "original_2 = \"You don't like pop culture references movies, unless they are heartfelt.\" #@param {type:\"string\"}\n",
    "original_3 = \"You don't like ghost story movies, especially if they are not inappropriate music.\" #@param {type:\"string\"}\n",
    "original_4 = \"You like bollywood movies, especially if they are not fighting the system.\" #@param {type:\"string\"}\n",
    "original_5 = \"You like nasa movies if they are identity theft.\"\n",
    "original_6 = \"You don't like pixar animation movies, especially if they are not halloween theme.\"\n",
    "original_7 = \"You like modern fantasy movies movies, unless they are intelligent thriller.\"\n",
    "original_8 = \"You don't like scandal movies if they are artificial human.\"\n",
    "original_9 = \"You like beautiful cinematography movies, especially if they are vistavision.\"\n",
    "original_10 = \"You don't like tarantino movies, unless they are interesting concept.\"\n",
    "original_11 = \"You like anarchy movies, especially if they are not neo-noir.\"\n",
    "\n",
    "original_12 = \"You don't like heartwarming movies if they are action thriller.\"\n",
    "original_13 = \"You don't like colourful movies if they are female power.\"\n",
    "original_14 = \"You don't like british comedy movies, unless they are fast-paced.\"\n",
    "original_15 = \"You don't like action thriller movies if they are espionage.\"\n",
    "original_16 = \"You like clever plot movies if they are dream within a dream.\"\n",
    "original_17 = \"You don’t like romantic comedy movies, especially if they are idiotic.\"\n",
    "original_18 = \"You like brilliant movies, especially if they are not poor script.\"\n",
    "original_19 = \"You like musical movies if they are hillarious.\"\n",
    "original_20 = \"You don’t like personality disorder movies, especially if they are no chemistry.\"\n",
    "fun1 = \"The birds are flying in the blue sky\"\n",
    "fun2 = \"I hear a very lovely piano sound from the next room\"\n",
    "fun3 = \"My dog is chasing neighbor's cat\"\n",
    "fun4 = \"The wolf (Canis lupus) is a large canine native to Eurasia and North America\"\n",
    "fun5 = \"SpaceX's Crew Dragon and Falcon 9 make their first crewed launch for NASA\"\n",
    "\n",
    "test_data = [original_1,original_2,original_3,original_4,original_5,original_6,original_7,original_8,original_9,original_10,original_11,\n",
    "             original_12,original_13,original_14,original_15,original_16,original_17,original_18,original_19,original_20, fun1, fun2, fun3,fun4,fun5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1534578,
     "status": "ok",
     "timestamp": 1591642919203,
     "user": {
      "displayName": "Renny Octavia Tan",
      "photoUrl": "",
      "userId": "17487398813778950254"
     },
     "user_tz": -120
    },
    "id": "QvGOt1X9JjoL",
    "outputId": "43a28e1b-7de2-4d47-9626-c6b366b23851"
   },
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"3B\"\n",
    "\n",
    "# insert the name of the models to generate paraphrase\n",
    "list_tuned_model_name =[\"3B_mixed_allMscoco_50kTemplate_ordered_train_temp1_beam1_50\",\n",
    "                        \"3B_mixed_allMscoco_50kTemplate_ordered_train_temp1_beam1_100\",\n",
    "                        \"3B_mixed_allMscoco_50kTemplate_ordered_train_temp1_beam1_150\",\n",
    "                        \"3B_mixed_allMscoco_50kTemplate_ordered_train_temp1_beam1_200\",\n",
    "                        \"3B_mixed_allMscoco_50kTemplate_ordered_train_temp1_beam1_250\"]\n",
    "\n",
    "predictions = {}\n",
    "for tuned_model_name in list_tuned_model_name:\n",
    "\n",
    "  #tuned_model_name = \"3B_mixed_allMscoco_50kTemplate_ordered_train_temp1_beam1_200\"\n",
    "  TUNED_MODELS_DIR = os.path.join(BASE_DIR, tuned_model_name)\n",
    "  LOAD_MODEL_DIR = os.path.join(TUNED_MODELS_DIR, MODEL_SIZE)\n",
    "  temp = 1\n",
    "  beam = 1\n",
    "\n",
    "\n",
    "  # Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
    "    # Limit number of checkpoints to fit within 5GB (if possible).\n",
    "  model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
    "        \"small\": (1, 256, 16),\n",
    "        \"base\": (2, 128, 8),\n",
    "        \"large\": (8, 64, 4),\n",
    "        \"3B\": (8, 16, 1),\n",
    "        \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
    "\n",
    "  model = t5.models.MtfModel(\n",
    "        model_dir=LOAD_MODEL_DIR,\n",
    "        tpu=TPU_ADDRESS,\n",
    "        tpu_topology=TPU_TOPOLOGY,\n",
    "        model_parallelism=model_parallelism,\n",
    "        batch_size=train_batch_size,\n",
    "        sequence_length={\"inputs\": 128, \"targets\": 128},\n",
    "        learning_rate_schedule=0.003,\n",
    "        save_checkpoints_steps=1000,\n",
    "        keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
    "        iterations_per_loop=100,\n",
    "    )\n",
    "\n",
    "\n",
    "  print(\"##############################Prediction for model {} ################################\".format(tuned_model_name))\n",
    "  #Predict based on test data and save to pickle\n",
    "  now = time.time()\n",
    "  # Write out the supplied questions to text files.\n",
    "\n",
    "  predict_inputs_path = os.path.join(LOAD_MODEL_DIR,\"playaround/\", tuned_model_name+\"_predict_inputs_%d.txt\" % now)\n",
    "  predict_outputs_path = os.path.join(LOAD_MODEL_DIR,\"playaroud/\", tuned_model_name+ \"_predict_outputs_%d.txt\" % now)\n",
    "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
    "  with tf.io.gfile.GFile(predict_inputs_path, \"w\") as f:\n",
    "    for q in test_data:\n",
    "      f.write(\"Original sentence: %s\\n\" % q.lower())\n",
    "      # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
    "    \n",
    "  # Ignore any logging so that we only see the model's answers to the questions.\n",
    "  with tf_verbosity_level('ERROR'):\n",
    "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
    "    model.predict(\n",
    "      input_file=predict_inputs_path,\n",
    "      output_file=predict_outputs_path,\n",
    "      # Select the most probable output token at each step.\n",
    "      beam_size=beam,\n",
    "      temperature=float(temp),\n",
    "    )\n",
    "\n",
    "  predictions[tuned_model_name] = {}\n",
    "  # The output filename will have the checkpoint appended so we glob to get \n",
    "  # the latest.\n",
    "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
    "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])  \n",
    "  with tf.io.gfile.GFile(prediction_files[-1]) as f:\n",
    "    for q, a in zip(test_data, f):\n",
    "      if q:\n",
    "        predictions[tuned_model_name][q] = a\n",
    "        print(\"original: \" + q)\n",
    "        print(\"paraphrase: \" + a)\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YFNlrK0Nx79"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "def dump_pickle(filename,obj):\n",
    "    with tf.io.gfile.GFile(filename,'wb') as f:\n",
    "        pickle.dump(obj,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4EAc7KEP09M"
   },
   "outputs": [],
   "source": [
    "generated_text_path = os.path.join(DATA_DIR,'generated_text/','playaround/')\n",
    "dump_pickle(generated_text_path+\"predictions_small_steps_ordered_beam1_temp1_second.pkl\",predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "extULGiDQ9HC"
   },
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "predictions[\"3B_mixed_allMscoco_50kTemplate_ordered_train_temp1_beam1_50\"]=1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyO3BZcc32Po/sQdgSL3YUcA",
   "collapsed_sections": [],
   "name": "Copy of Generate_paraphrase.ipynb",
   "provenance": [
    {
     "file_id": "1Q1SvyiU8m4J43DZLyBFihWyYd2jZdqhU",
     "timestamp": 1592330035446
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
