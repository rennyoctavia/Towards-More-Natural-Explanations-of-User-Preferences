{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98210,
     "status": "ok",
     "timestamp": 1591774921002,
     "user": {
      "displayName": "Renny Octavia Tan",
      "photoUrl": "",
      "userId": "17487398813778950254"
     },
     "user_tz": -120
    },
    "id": "Khz2Aphk27Id",
    "outputId": "8efc5902-d4ca-479a-a8d6-84267340c920"
   },
   "source": [
    "# T5 - Fine tuning for Paraphrasing\n",
    "\n",
    "We modified code from https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb#scrollTo=zrtR2urJV3ST for paraphrasing, which is originally a tutorial for fine-tuning pre-trained T5 for closed book question by the T5 author.\n",
    "\n",
    "This notebook is to be run in Google Colab using TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "%tensorflow_version 2.x\n",
    "!pip install -q t5\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import t5\n",
    "\n",
    "BASE_DIR = \"gs://t5_renny_new\" #@param { type: \"string\" }\n",
    "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
    "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data_paraphrase\")\n",
    "ON_CLOUD = True\n",
    "\n",
    "\n",
    "if ON_CLOUD:\n",
    "  print(\"Setting up GCS access...\")\n",
    "  import tensorflow_gcs_config\n",
    "  from google.colab import auth\n",
    "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
    "  TPU_TOPOLOGY = \"2x2\"\n",
    "  try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    TPU_ADDRESS = tpu.get_master()\n",
    "    print('Running on TPU:', TPU_ADDRESS)\n",
    "  except ValueError:\n",
    "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "  auth.authenticate_user()\n",
    "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
    "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Improve logging.\n",
    "from contextlib import contextmanager\n",
    "import logging as py_logging\n",
    "\n",
    "if ON_CLOUD:\n",
    "  tf.get_logger().propagate = False\n",
    "  py_logging.root.setLevel('INFO')\n",
    "\n",
    "@contextmanager\n",
    "def tf_verbosity_level(level):\n",
    "  og_level = tf.logging.get_verbosity()\n",
    "  tf.logging.set_verbosity(level)\n",
    "  yield\n",
    "  tf.logging.set_verbosity(og_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azBD3sD1O2bV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def dump_pickle(filename,obj):\n",
    "    with tf.io.gfile.GFile(filename,'wb') as f:\n",
    "        pickle.dump(obj,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMEqldKuZYh7"
   },
   "outputs": [],
   "source": [
    "generated_text_path = os.path.join(DATA_DIR,'generated_text/')\n",
    "\n",
    "#parameters\n",
    "PR_SPLIT_FNAMES = {\n",
    "    \"train\": \"mscoco_train.txt\",\n",
    "    \"validation\": \"mscoco_val.txt\"\n",
    "}\n",
    "pr_counts_path = os.path.join(DATA_DIR, \"mscoco-counts.json\")\n",
    "pr_tsv_path = {\n",
    "    \"train\": os.path.join(DATA_DIR, \"mscoco_train.tsv\"),\n",
    "    \"validation\": os.path.join(DATA_DIR, \"mscoco_val.tsv\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLH4iMVBAr5H"
   },
   "source": [
    "Uploading file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text files to TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14824,
     "status": "ok",
     "timestamp": 1591775020820,
     "user": {
      "displayName": "Renny Octavia Tan",
      "photoUrl": "",
      "userId": "17487398813778950254"
     },
     "user_tz": -120
    },
    "id": "PASOFzWFAwBj",
    "outputId": "1bd42b1e-81f9-4766-933c-17c307a6fbdf"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "# Public directory of Natural Questions data on GCS.\n",
    "#NQ_JSONL_DIR = \"gs://natural_questions/v1.0-simplified/\"\n",
    "\n",
    "def nq_jsonl_to_tsv(in_fname, out_fname):\n",
    "\n",
    "  def extract_answer(tokens, span):\n",
    "    \"\"\"Reconstruct answer from token span and remove extra spaces.\"\"\"\n",
    "    start, end = span[\"start_token\"], span[\"end_token\"]  \n",
    "    ans = \" \".join(tokens[start:end])\n",
    "    # Remove incorrect spacing around punctuation.\n",
    "    ans = ans.replace(\" ,\", \",\").replace(\" .\", \".\").replace(\" %\", \"%\")\n",
    "    ans = ans.replace(\" - \", \"-\").replace(\" : \", \":\").replace(\" / \", \"/\")\n",
    "    ans = ans.replace(\"( \", \"(\").replace(\" )\", \")\")\n",
    "    ans = ans.replace(\"`` \", \"\\\"\").replace(\" ''\", \"\\\"\")\n",
    "    ans = ans.replace(\" 's\", \"'s\").replace(\"s ' \", \"s' \")\n",
    "    return ans\n",
    "\n",
    "  count = 0\n",
    "  with tf.io.gfile.GFile(in_fname, \"r\") as infile,\\\n",
    "       tf.io.gfile.GFile(out_fname, \"w\") as outfile:\n",
    "    for line in infile:\n",
    "      line = line.replace('\\n','')\n",
    "      if line != \"<|end of text|>\":\n",
    "        line = line.split(\">>>>>>\")\n",
    "        original = line[0]\n",
    "        paraphrase = line[1]\n",
    "        \n",
    "        # Write this line as <question>\\t<answer>\n",
    "        outfile.write(\"%s\\t%s\\n\" % (original, paraphrase))\n",
    "        count += 1\n",
    "        tf.logging.log_every_n(\n",
    "            tf.logging.INFO,\n",
    "            \"Wrote %d examples to %s.\" % (count, out_fname),\n",
    "            1000)\n",
    "    return count\n",
    "\n",
    "if tf.io.gfile.exists(pr_counts_path):\n",
    "  # Used cached data and counts.\n",
    "  tf.logging.info(\"Loading NQ from cache.\")\n",
    "  num_nq_examples = json.load(tf.io.gfile.GFile(pr_counts_path))\n",
    "else:\n",
    "  # Create TSVs and get counts.\n",
    "  tf.logging.info(\"Generating Ori-Para TSVs.\")\n",
    "  num_nq_examples = {}\n",
    "  for split, fname in PR_SPLIT_FNAMES.items():\n",
    "    print(split)\n",
    "    print(fname)\n",
    "    num_nq_examples[split] = nq_jsonl_to_tsv(\n",
    "        os.path.join(DATA_DIR, fname), pr_tsv_path[split])\n",
    "  json.dump(num_nq_examples, tf.io.gfile.GFile(pr_counts_path, \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSB5YfT4K9My"
   },
   "source": [
    "## Function to load the TSV data as a tf.data.Dataset in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1336,
     "status": "ok",
     "timestamp": 1591775139775,
     "user": {
      "displayName": "Renny Octavia Tan",
      "photoUrl": "",
      "userId": "17487398813778950254"
     },
     "user_tz": -120
    },
    "id": "biuCEzEOK0BW",
    "outputId": "85534c95-4573-4f53-b725-d24050aaf68e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few raw validation examples...\n",
      "{'original': b'the living room of a home with a large sectional couch', 'paraphrase': b'a very net living room with a big  peace couch under a large rug'}\n",
      "{'original': b'a very net living room with a big  peace couch under a large rug', 'paraphrase': b'the living room of a home with a large sectional couch'}\n",
      "{'original': b'a l shaped couch in the living room', 'paraphrase': b'a large sectional sofa is sitting to the far left of a television screen'}\n",
      "{'original': b'a large sectional sofa is sitting to the far left of a television screen', 'paraphrase': b'a l shaped couch in the living room'}\n",
      "{'original': b'a man wearing headphones while chewing on an ink pen', 'paraphrase': b'a man wearing headphones with a toothbrush in his mouth'}\n"
     ]
    }
   ],
   "source": [
    "def pr_dataset_fn(split, shuffle_files=False):\n",
    "  # We only have one file for each split.\n",
    "  del shuffle_files\n",
    "\n",
    "  # Load lines from the text file as examples.\n",
    "  ds = tf.data.TextLineDataset(pr_tsv_path[split])\n",
    "  # Split each \"<original>\\t<paraphrase>\" example into (original, paraphrase) tuple.\n",
    "  ds = ds.map(\n",
    "      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
    "                        field_delim=\"\\t\", use_quote_delim=False),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  # Map each tuple to a {\"original\": ... \"paraphrase\": ...} dict.\n",
    "  ds = ds.map(lambda *ex: dict(zip([\"original\", \"paraphrase\"], ex)))\n",
    "  return ds\n",
    "\n",
    "print(\"A few raw validation examples...\")\n",
    "for ex in tfds.as_numpy(pr_dataset_fn(\"validation\").take(5)):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tspVvvQqNS_A"
   },
   "outputs": [],
   "source": [
    "def sentence_preprocessor(ds):\n",
    "  def normalize_text(text):\n",
    "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n",
    "    return text\n",
    "\n",
    "  def to_inputs_and_targets(ex):\n",
    "    \"\"\"Map {\"question\": ..., \"answer\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
    "    return {\n",
    "        \"inputs\":\n",
    "             tf.strings.join(\n",
    "                 [\"original sentence: \", normalize_text(ex[\"original\"])]),\n",
    "        \"targets\": normalize_text(ex[\"paraphrase\"])\n",
    "    }\n",
    "  return ds.map(to_inputs_and_targets, \n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register task to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMbkI3O7N9LJ"
   },
   "outputs": [],
   "source": [
    "t5.data.TaskRegistry.remove(\"paraphrasing\")\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"paraphrasing\",\n",
    "    # Supply a function which returns a tf.data.Dataset.\n",
    "    dataset_fn=pr_dataset_fn,\n",
    "    splits=[\"train\", \"validation\"],\n",
    "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
    "    text_preprocessor=[sentence_preprocessor],\n",
    "    # Lowercase targets before computing metrics.\n",
    "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
    "    # We'll use accuracy as our evaluation metric.\n",
    "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
    "    # Not required, but helps for mixing and auto-caching.\n",
    "    num_input_examples=num_nq_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1555,
     "status": "ok",
     "timestamp": 1591775195088,
     "user": {
      "displayName": "Renny Octavia Tan",
      "photoUrl": "",
      "userId": "17487398813778950254"
     },
     "user_tz": -120
    },
    "id": "fIo031eeOFd6",
    "outputId": "cf2d14df-8315-44ca-cdf3-98f2a2888bfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few preprocessed validation examples...\n",
      "{'inputs_plaintext': b'original sentence: a woman wearing a multi colored  striped sweater holds her arms up triumphantly as a kite flies high in the sky', 'inputs': array([  926,  7142,    10,     3,     9,  2335,  5119,     3,     9,\n",
      "        1249, 11999,     3, 27465, 19469,  4532,   160,  6026,    95,\n",
      "       20020,   288,   120,    38,     3,     9,  3650,    15,     3,\n",
      "          89,  4664,   306,    16,     8,  5796,     1]), 'targets_plaintext': b'a woman flying a kite in the blue cloud filled sky', 'targets': array([   3,    9, 2335, 7070,    3,    9, 3650,   15,   16,    8, 1692,\n",
      "       3126, 3353, 5796,    1])}\n",
      "{'inputs_plaintext': b'original sentence: a large area rug lines the floor of a room', 'inputs': array([ 926, 7142,   10,    3,    9,  508,  616, 9787, 2356,    8, 1501,\n",
      "         13,    3,    9,  562,    1]), 'targets_plaintext': b'a room with a red and white carpet some blue drums and a piano', 'targets': array([   3,    9,  562,   28,    3,    9, 1131,   11,  872, 4898,  128,\n",
      "       1692, 5253,    7,   11,    3,    9, 8355,    1])}\n",
      "{'inputs_plaintext': b'original sentence: the glass mug is holding two  new toothbrushes', 'inputs': array([  926,  7142,    10,     8,  1905,     3, 12963,    19,  3609,\n",
      "         192,   126, 30002,    15,     7,     1]), 'targets_plaintext': b'two toothbrushes sitting inside a grungy glass on a counter top', 'targets': array([  192, 30002,    15,     7,  3823,  1096,     3,     9,  3542,\n",
      "         425,    63,  1905,    30,     3,     9,  3485,   420,     1])}\n",
      "{'inputs_plaintext': b'original sentence: two kites flying above a smokestack with a town on a hill in the background', 'inputs': array([ 926, 7142,   10,  192, 3650,   15,    7, 7070,  756,    3,    9,\n",
      "       7269,    7,   17, 4365,   28,    3,    9, 1511,   30,    3,    9,\n",
      "       9956,   16,    8, 2458,    1]), 'targets_plaintext': b'two kites flying in the sky next to a smoke stack', 'targets': array([ 192, 3650,   15,    7, 7070,   16,    8, 5796,  416,   12,    3,\n",
      "          9, 7269, 9013,    1])}\n",
      "{'inputs_plaintext': b'original sentence: a  shadow of a person flying a kite on a sandy beach', 'inputs': array([  926,  7142,    10,     3,     9,  8552,    13,     3,     9,\n",
      "         568,  7070,     3,     9,  3650,    15,    30,     3,     9,\n",
      "       23151,  2608,     1]), 'targets_plaintext': b'shadows of an individual and a flying flag on the sandy beach', 'targets': array([ 8552,     7,    13,    46,   928,    11,     3,     9,  7070,\n",
      "        5692,    30,     8, 23151,  2608,     1])}\n"
     ]
    }
   ],
   "source": [
    "pr_task = t5.data.TaskRegistry.get(\"paraphrasing\")\n",
    "ds = pr_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 128, \"targets\": 128})\n",
    "print(\"A few preprocessed validation examples...\")\n",
    "for ex in tfds.as_numpy(ds.take(5)):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0XPOUH6cBl2"
   },
   "outputs": [],
   "source": [
    "#Sentences generated from test split of tags\n",
    "\n",
    "original_1 = \"You like robbery movies, especially if they are keri russell.\" \n",
    "original_2 = \"You don't like pop culture references movies, unless they are heartfelt.\" \n",
    "original_3 = \"You don't like ghost story movies, especially if they are not inappropriate music.\" \n",
    "original_4 = \"You like bollywood movies, especially if they are not fighting the system.\" \n",
    "original_5 = \"You like nasa movies if they are identity theft.\"\n",
    "original_6 = \"You don't like pixar animation movies, especially if they are not halloween theme.\"\n",
    "original_7 = \"You like modern fantasy movies movies, unless they are intelligent thriller.\"\n",
    "original_8 = \"You don't like scandal movies if they are artificial human.\"\n",
    "original_9 = \"You like beautiful cinematography movies, especially if they are vistavision.\"\n",
    "original_10 = \"You don't like tarantino movies, unless they are interesting concept.\"\n",
    "original_11 = \"You like anarchy movies, especially if they are not neo-noir.\"\n",
    "\n",
    "original_12 = \"You don't like heartwarming movies if they are action thriller.\"\n",
    "original_13 = \"You don't like colourful movies if they are female power.\"\n",
    "original_14 = \"You don't like british comedy movies, unless they are fast-paced.\"\n",
    "original_15 = \"You don't like action thriller movies if they are espionage.\"\n",
    "original_16 = \"You like clever plot movies if they are dream within a dream.\"\n",
    "original_17 = \"You don’t like romantic comedy movies, especially if they are idiotic.\"\n",
    "original_18 = \"You like brilliant movies, especially if they are not poor script.\"\n",
    "original_19 = \"You like musical movies if they are hillarious.\"\n",
    "original_20 = \"You don’t like personality disorder movies, especially if they are no chemistry.\"\n",
    "fun1 = \"The birds are flying in the blue sky\"\n",
    "fun2 = \"I hear a very lovely piano sound from the next room\"\n",
    "fun3 = \"My dog is chasing neighbor's cat\"\n",
    "fun4 = \"The wolf (Canis lupus) is a large canine native to Eurasia and North America\"\n",
    "fun5 = \"SpaceX's Crew Dragon and Falcon 9 make their first crewed launch for NASA\"\n",
    "\n",
    "test_data = [original_1,original_2,original_3,original_4,original_5,original_6,original_7,original_8,original_9,original_10,original_11,\n",
    "             original_12,original_13,original_14,original_15,original_16,original_17,original_18,original_19,original_20, fun1, fun2, fun3,fun4,fun5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning from train data and generate paraphrase from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5151572,
     "status": "ok",
     "timestamp": 1591780428693,
     "user": {
      "displayName": "Renny Octavia Tan",
      "photoUrl": "",
      "userId": "17487398813778950254"
     },
     "user_tz": -120
    },
    "id": "ROOuUAYwOc7A",
    "outputId": "35b9c7f6-2d3a-472f-d4a9-7bb6bfd3e4d5"
   },
   "outputs": [],
   "source": [
    "#MODELS_DIR = os.path.join(BASE_DIR, \"models_paraphrase\")\n",
    "MODEL_SIZE = \"3B\" #@param[\"small\", \"base\", \"large\", \"3B\", \"11B\"]\n",
    "# Public GCS path for T5 pre-trained model checkpoints\n",
    "BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
    "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
    "\n",
    "FINETUNE_STEPS_LIST = [15,200,1000,1500] \n",
    "temp = 1 #must be 0 if beam size = >1\n",
    "beam_size = 1\n",
    "predictions = {}\n",
    "for FINETUNE_STEPS in FINETUNE_STEPS_LIST:\n",
    "  #Run_name\n",
    "  model_name = MODEL_SIZE+\"_metr_acc\"+\"_\"+PR_SPLIT_FNAMES[\"train\"].split(\".\")[0]+\"_temp\"+str(temp)+\"_beam\"+str(beam_size)+\"_\"+str(FINETUNE_STEPS)\n",
    "  MODELS_DIR = os.path.join(BASE_DIR, model_name)\n",
    "  MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)\n",
    "  \n",
    "  predictions[model_name] = {}\n",
    "\n",
    "  if ON_CLOUD and MODEL_SIZE == \"3B\":\n",
    "    tf.logging.warn(\n",
    "        \"The `3B` model is too large to use with the 5GB GCS free tier. \"\n",
    "        \"Make sure you have at least 25GB on GCS before continuing.\"\n",
    "    )\n",
    "  elif ON_CLOUD and MODEL_SIZE == \"11B\":\n",
    "    raise ValueError(\n",
    "        \"The `11B` parameter is too large to fine-tune on the `v2-8` TPU \"\n",
    "        \"provided by Colab. Please comment out this Error if you're running \"\n",
    "        \"on a larger TPU.\"\n",
    "    )\n",
    "\n",
    "  # Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
    "  # Limit number of checkpoints to fit within 5GB (if possible).\n",
    "  model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
    "      \"small\": (1, 256, 16),\n",
    "      \"base\": (2, 128, 8),\n",
    "      \"large\": (8, 64, 4),\n",
    "      \"3B\": (8, 16, 1),\n",
    "      \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
    "\n",
    "  tf.io.gfile.makedirs(MODEL_DIR)\n",
    "  # The models from our paper are based on the Mesh Tensorflow Transformer.\n",
    "  model = t5.models.MtfModel(\n",
    "      model_dir=MODEL_DIR,\n",
    "      tpu=TPU_ADDRESS,\n",
    "      tpu_topology=TPU_TOPOLOGY,\n",
    "      model_parallelism=model_parallelism,\n",
    "      batch_size=train_batch_size,\n",
    "      sequence_length={\"inputs\": 128, \"targets\": 128},\n",
    "      learning_rate_schedule=0.003,\n",
    "      save_checkpoints_steps=1000,\n",
    "      keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
    "      iterations_per_loop=100,\n",
    "  )\n",
    "\n",
    "  print(\"##############################Fine tuning for model {} ################################\".format(model_name))\n",
    "  #fine tune model\n",
    "  model.finetune(\n",
    "      mixture_or_task_name=\"paraphrasing\",\n",
    "      pretrained_model_dir=PRETRAINED_DIR,\n",
    "      #pretrained_checkpoint_step = 1000120,\n",
    "      finetune_steps=FINETUNE_STEPS\n",
    "  )\n",
    "\n",
    "\n",
    "  print(\"##############################Prediction for model {} ################################\".format(model_name))\n",
    "  #Predict based on test data and save to pickle\n",
    "  now = time.time()\n",
    "  # Write out the supplied questions to text files.\n",
    "\n",
    "  predict_inputs_path = os.path.join(MODEL_DIR, model_name+\"_predict_inputs_%d.txt\" % now)\n",
    "  predict_outputs_path = os.path.join(MODEL_DIR, model_name+ \"_predict_outputs_%d.txt\" % now)\n",
    "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
    "  with tf.io.gfile.GFile(predict_inputs_path, \"w\") as f:\n",
    "    for q in test_data:\n",
    "      f.write(\"Original sentence: %s\\n\" % q.lower())\n",
    "    # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
    "  \n",
    "  # Ignore any logging so that we only see the model's answers to the questions.\n",
    "  with tf_verbosity_level('ERROR'):\n",
    "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
    "    model.predict(\n",
    "      input_file=predict_inputs_path,\n",
    "      output_file=predict_outputs_path,\n",
    "      # Select the most probable output token at each step.\n",
    "      beam_size=beam_size,\n",
    "      temperature=float(temp),\n",
    "    )\n",
    "\n",
    "\n",
    "  # The output filename will have the checkpoint appended so we glob to get \n",
    "  # the latest.\n",
    "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
    "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])  \n",
    "  with tf.io.gfile.GFile(prediction_files[-1]) as f:\n",
    "    for q, a in zip(test_data, f):\n",
    "      if q:\n",
    "        predictions[model_name][q] = a\n",
    "        print(\"original: \" + q)\n",
    "        print(\"paraphrase: \" + a)\n",
    "        print()\n",
    "\n",
    "# saving all generated paraphrase in pickle file, with model name as key\n",
    "dump_pickle(generated_text_path+\"predictions_ordered_beam1_temp1_mscoco_only.pkl\",predictions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNEWS/fNZiQcooA8ZHiHqZE",
   "collapsed_sections": [],
   "name": "Copy of Copy of T5_paraphrasing.ipynb",
   "provenance": [
    {
     "file_id": "1-58A1KekADI8AzauEfkfRNlUNVouldn7",
     "timestamp": 1592329537888
    },
    {
     "file_id": "1yKpXdF7DAQ64yeICmYzbkkK7W-akD38U",
     "timestamp": 1591358918915
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
